#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥–åŠ±æ¨¡å‹åˆ†ç±»æ•°æ®æ„é€ å’Œå¤„ç†ç³»ç»Ÿï¼ˆé‡æ„ç‰ˆï¼‰
ç”¨äºæ„å»º5ç±»é£æ ¼çš„åˆ†ç±»æ•°æ®é›†ï¼šå”®å‰ã€å”®åã€è¯šå®ã€æœ‰å¸®åŠ©ã€æ— ä¼¤å®³

ç‰¹ç‚¹ï¼š
- æ•°æ®å’Œä»£ç åˆ†ç¦»ï¼šåŸå§‹æ•°æ®å­˜å‚¨ä¸ºJSONæ–‡ä»¶
- ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼šä¾¿äºé¡¹ç›®è¿ç§»å’Œç»´æŠ¤
- é…ç½®åŒ–ç®¡ç†ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†æ˜ å°„å…³ç³»å’Œå‚æ•°

æ•°æ®æµç¨‹ï¼š
1. ä»JSONæ–‡ä»¶åŠ è½½ä¸šåŠ¡æ•°æ®å’Œå¼€æºæ•°æ®
2. åœºæ™¯æ ‡ç­¾æ˜ å°„åˆ°é£æ ¼ç»´åº¦
3. è´¨é‡ç­›é€‰ï¼ˆä¿ç•™4æ˜Ÿä»¥ä¸Šï¼‰
4. å¯¹è¯æ‹†åˆ†
5. æ•°æ®é‡å†™å’Œæ¸…æ´—
6. å»é‡å¤„ç†
7. æœ€ç»ˆæ•°æ®é›†æ•´åˆ
8. æŒ‰é£æ ¼ä¿å­˜ç‹¬ç«‹æ•°æ®æ–‡ä»¶
"""

import json
import os
import random
import re
from typing import List, Dict, Tuple
from collections import defaultdict, Counter
import hashlib
from datetime import datetime
from pathlib import Path


class RewardModelDataProcessor:
    """å¥–åŠ±æ¨¡å‹æ•°æ®å¤„ç†å™¨ï¼ˆé‡æ„ç‰ˆï¼‰"""
    
    def __init__(self, config_path: str = "rm_data/input_data/config.json"):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        """
        self.config_path = config_path
        self.config = self.load_config()
        
        # ä»é…ç½®æ–‡ä»¶ä¸­åŠ è½½æ˜ å°„å…³ç³»
        self.scene_to_style_mapping = self.config["scene_to_style_mapping"]
        self.style_to_english = self.config["style_to_english"]
        self.processing_settings = self.config["processing_settings"]
        
        # è®¾ç½®è·¯å¾„
        self.input_data_dir = self.config["paths"]["input_data_dir"]
        self.output_data_dir = self.config["paths"]["output_data_dir"]
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(self.output_data_dir).mkdir(parents=True, exist_ok=True)
        
        print("=" * 60)
        print("ğŸš€ å¥–åŠ±æ¨¡å‹æ•°æ®å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼ˆé‡æ„ç‰ˆï¼‰")
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {self.input_data_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_data_dir}")
        print("=" * 60)
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
            return config
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path}")
            raise
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
    
    def load_business_data(self) -> List[Dict]:
        """
        æ­¥éª¤1: ä»JSONæ–‡ä»¶åŠ è½½ä¸šåŠ¡æ•°æ®
        """
        print("\nğŸ“Š æ­¥éª¤1: ä»æ–‡ä»¶åŠ è½½ä¸šåŠ¡æ•°æ®...")
        
        business_file = os.path.join(self.input_data_dir, self.config["paths"]["business_data_file"])
        
        try:
            with open(business_file, 'r', encoding='utf-8') as f:
                business_json = json.load(f)
            
            conversations = business_json["conversations"]
            print(f"âœ… æˆåŠŸåŠ è½½ä¸šåŠ¡æ•°æ®æ–‡ä»¶: {business_file}")
            print(f"   å…± {len(conversations)} æ¡å¯¹è¯æ•°æ®")
            
            # ç»Ÿè®¡åœºæ™¯åˆ†å¸ƒ
            scene_counts = Counter([conv['scene'] for conv in conversations])
            print("   åœºæ™¯åˆ†å¸ƒ:")
            for scene, count in scene_counts.items():
                print(f"     {scene}: {count} æ¡")
            
            return conversations
            
        except FileNotFoundError:
            print(f"âŒ ä¸šåŠ¡æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {business_file}")
            raise
        except Exception as e:
            print(f"âŒ åŠ è½½ä¸šåŠ¡æ•°æ®æ—¶å‡ºé”™: {e}")
            raise
    
    def load_opensource_data(self) -> List[Dict]:
        """
        æ­¥éª¤2: ä»JSONæ–‡ä»¶åŠ è½½å¼€æºæ•°æ®
        """
        print("\nğŸ“š æ­¥éª¤2: ä»æ–‡ä»¶åŠ è½½å¼€æºæ•°æ®...")
        
        opensource_file = os.path.join(self.input_data_dir, self.config["paths"]["opensource_data_file"])
        
        try:
            with open(opensource_file, 'r', encoding='utf-8') as f:
                opensource_json = json.load(f)
            
            samples = opensource_json["samples"]
            print(f"âœ… æˆåŠŸåŠ è½½å¼€æºæ•°æ®æ–‡ä»¶: {opensource_file}")
            print(f"   å…± {len(samples)} æ¡æ ·æœ¬æ•°æ®")
            
            # ç»Ÿè®¡é£æ ¼åˆ†å¸ƒ
            style_counts = Counter([sample['style'] for sample in samples])
            print("   é£æ ¼åˆ†å¸ƒ:")
            for style, count in style_counts.items():
                print(f"     {style}: {count} æ¡")
            
            return samples
            
        except FileNotFoundError:
            print(f"âŒ å¼€æºæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {opensource_file}")
            raise
        except Exception as e:
            print(f"âŒ åŠ è½½å¼€æºæ•°æ®æ—¶å‡ºé”™: {e}")
            raise
    
    def map_scenes_to_styles(self, business_data: List[Dict]) -> List[Dict]:
        """
        æ­¥éª¤3: åœºæ™¯æ ‡ç­¾æ˜ å°„åˆ°é£æ ¼ç»´åº¦
        å°†ä¸šåŠ¡åœºæ™¯æ ‡ç­¾æ˜ å°„åˆ°5ç±»é£æ ¼ç»´åº¦
        """
        print("\nğŸ”„ æ­¥éª¤3: åœºæ™¯æ ‡ç­¾æ˜ å°„åˆ°é£æ ¼ç»´åº¦...")
        
        mapped_data = []
        unmapped_scenes = []
        
        for data in business_data:
            scene = data['scene']
            if scene in self.scene_to_style_mapping:
                mapped_style = self.scene_to_style_mapping[scene]
                data['style'] = mapped_style
                data['style_english'] = self.style_to_english[mapped_style]
                mapped_data.append(data)
            else:
                unmapped_scenes.append(scene)
                print(f"âš ï¸  æœªæ‰¾åˆ°åœºæ™¯ '{scene}' çš„æ˜ å°„ï¼Œè·³è¿‡")
        
        if unmapped_scenes:
            print(f"âš ï¸  å…±æœ‰ {len(unmapped_scenes)} ä¸ªåœºæ™¯æœªæ˜ å°„: {set(unmapped_scenes)}")
        
        print(f"âœ… åœºæ™¯æ˜ å°„å®Œæˆï¼Œå…± {len(mapped_data)} æ¡æ•°æ®")
        style_counts = Counter([d['style'] for d in mapped_data])
        print("   æ˜ å°„åé£æ ¼åˆ†å¸ƒ:")
        for style, count in style_counts.items():
            print(f"     {style}: {count} æ¡")
            
        return mapped_data
    
    def quality_filtering(self, data: List[Dict], data_type: str = "business") -> List[Dict]:
        """
        æ­¥éª¤4: è´¨é‡ç­›é€‰
        åªä¿ç•™ç”¨æˆ·åé¦ˆæ¯”è¾ƒé«˜çš„æ•°æ®
        """
        min_score = self.processing_settings["min_score"]
        print(f"\nğŸ” æ­¥éª¤4: è´¨é‡ç­›é€‰-{data_type}ï¼ˆä¿ç•™ >= {min_score} æ˜Ÿè¯„åˆ†ï¼‰...")
        
        filtered_data = []
        for item in data:
            # ä¸šåŠ¡æ•°æ®æœ‰user_satisfactionå­—æ®µ
            if 'user_satisfaction' in item:
                if item['user_satisfaction'] >= min_score:
                    filtered_data.append(item)
            # å¼€æºæ•°æ®æœ‰scoreå­—æ®µ
            elif 'score' in item:
                if item['score'] >= min_score:
                    filtered_data.append(item)
        
        filter_rate = len(filtered_data) / len(data) * 100 if len(data) > 0 else 0
        print(f"âœ… è´¨é‡ç­›é€‰å®Œæˆ")
        print(f"   åŸå§‹æ•°æ®: {len(data)} æ¡")  
        print(f"   ç­›é€‰å: {len(filtered_data)} æ¡")
        print(f"   ç­›é€‰ç‡: {filter_rate:.1f}%")
        
        return filtered_data
    
    def split_conversations(self, business_data: List[Dict]) -> List[Dict]:
        """
        æ­¥éª¤5: å¯¹è¯æ‹†åˆ†
        å°†å¤šè½®å¯¹è¯æ‹†åˆ†ä¸ºå•è½®é—®ç­”å¯¹
        """
        print("\nâœ‚ï¸  æ­¥éª¤5: å¯¹è¯æ‹†åˆ†...")
        
        split_data = []
        for data in business_data:
            conversations = data.get('conversations', [])
            for i, conv in enumerate(conversations):
                split_item = {
                    'original_dialogue_id': data['dialogue_id'],
                    'turn_id': f"{data['dialogue_id']}_turn_{i+1}",
                    'scene': data['scene'],
                    'style': data['style'],
                    'style_english': data['style_english'], 
                    'user_query': conv['user'],
                    'agent_reply': conv['agent'],
                    'user_satisfaction': data['user_satisfaction']
                }
                split_data.append(split_item)
        
        print(f"âœ… å¯¹è¯æ‹†åˆ†å®Œæˆ")
        print(f"   åŸå§‹å¯¹è¯: {len(business_data)} æ¡")
        print(f"   æ‹†åˆ†åé—®ç­”å¯¹: {len(split_data)} æ¡")
        
        return split_data
    
    def clean_and_rewrite_data(self, data: List[Dict], data_type: str = "business") -> List[Dict]:
        """
        æ­¥éª¤6: æ•°æ®æ¸…æ´—å’Œé‡å†™
        è§„èŒƒåŒ–é—®ç­”æ ¼å¼ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
        """
        print(f"\nğŸ§¹ æ­¥éª¤6: æ•°æ®æ¸…æ´—å’Œé‡å†™-{data_type}...")
        
        cleaned_data = []
        for item in data:
            # æ¨¡æ‹Ÿæ•°æ®æ¸…æ´—ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œè§„èŒƒåŒ–æ ¼å¼
            cleaned_item = item.copy()
            
            # æ¸…æ´—ç”¨æˆ·é—®é¢˜
            user_query = item['user_query']
            user_query = re.sub(r'[^\w\s\u4e00-\u9fffï¼Ÿï¼ï¼Œã€‚ï¼Ÿï¼]', '', user_query)
            user_query = user_query.strip()
            
            # æ¸…æ´—å®¢æœå›å¤  
            agent_reply = item['agent_reply']
            agent_reply = re.sub(r'[^\w\s\u4e00-\u9fffï¼Ÿï¼ï¼Œã€‚ï¼Ÿï¼ï½]', '', agent_reply)
            agent_reply = agent_reply.strip()
            
            # ç¡®ä¿å›å¤é•¿åº¦åˆç†
            if len(user_query) > 0 and len(agent_reply) > 0:
                cleaned_item['user_query'] = user_query
                cleaned_item['agent_reply'] = agent_reply
                cleaned_data.append(cleaned_item)
        
        print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ")  
        print(f"   æ¸…æ´—å‰: {len(data)} æ¡")
        print(f"   æ¸…æ´—å: {len(cleaned_data)} æ¡")
        
        return cleaned_data
    
    def deduplicate_data(self, data: List[Dict], data_type: str = "business") -> List[Dict]:
        """
        æ­¥éª¤7: å»é‡å¤„ç†
        ä½¿ç”¨ç²¾ç¡®å»é‡å’Œè¯­ä¹‰å»é‡
        """
        print(f"\nğŸ”„ æ­¥éª¤7: å»é‡å¤„ç†-{data_type}...")
        
        # ç¬¬ä¸€æ­¥ï¼šç²¾ç¡®å»é‡ï¼ˆä½¿ç”¨hashï¼‰
        seen_hashes = set()
        deduplicated_data = []
        
        for item in data:
            # åˆ›å»ºå†…å®¹çš„hash
            content = item['user_query'] + item['agent_reply']
            content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                deduplicated_data.append(item)
        
        # ç¬¬äºŒæ­¥ï¼šè¯­ä¹‰å»é‡ï¼ˆç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥ï¼‰
        duplicate_threshold = self.processing_settings["duplicate_threshold"]
        final_data = []
        
        for item in deduplicated_data:
            is_similar = False
            for existing_item in final_data:
                # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥
                if (item.get('style') == existing_item.get('style') and 
                    len(set(item['user_query']) & set(existing_item['user_query'])) > len(item['user_query']) * duplicate_threshold):
                    is_similar = True
                    break
            
            if not is_similar:
                final_data.append(item)
        
        print(f"âœ… å»é‡å®Œæˆ")
        print(f"   å»é‡å‰: {len(data)} æ¡")
        print(f"   ç²¾ç¡®å»é‡å: {len(deduplicated_data)} æ¡") 
        print(f"   è¯­ä¹‰å»é‡å: {len(final_data)} æ¡")
        
        return final_data
    
    def integrate_final_dataset(self, business_data: List[Dict], open_source_data: List[Dict]) -> Dict:
        """
        æ­¥éª¤8: æ•´åˆæœ€ç»ˆæ•°æ®é›†
        åˆå¹¶ä¸šåŠ¡æ•°æ®å’Œå¼€æºæ•°æ®ï¼Œå½¢æˆæœ€ç»ˆè®­ç»ƒæ•°æ®é›†
        """
        print("\nğŸ”— æ­¥éª¤8: æ•´åˆæœ€ç»ˆæ•°æ®é›†...")
        
        # å°†å¼€æºæ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        formatted_open_source = []
        for item in open_source_data:
            formatted_item = {
                'turn_id': f"opensource_{len(formatted_open_source)+1}",
                'scene': 'opensource',
                'style': item['style'] if item['style'] in ['helpful', 'harmless', 'honesty'] else 'helpful',
                'style_english': item['style'] if item['style'] in ['helpful', 'harmless', 'honesty'] else 'helpful',
                'user_query': item['user_query'],
                'agent_reply': item['agent_reply'],
                'score': item['score'],
                'source': 'opensource'
            }
            # æ˜ å°„styleåˆ°ä¸­æ–‡
            style_mapping = {'helpful': 'æœ‰å¸®åŠ©', 'harmless': 'æ— ä¼¤å®³', 'honesty': 'è¯šå®'}
            if formatted_item['style'] in style_mapping:
                formatted_item['style'] = style_mapping[formatted_item['style']]
            formatted_open_source.append(formatted_item)
        
        # ä¸ºä¸šåŠ¡æ•°æ®æ·»åŠ sourceæ ‡è¯†
        for item in business_data:
            item['source'] = 'business'
        
        # åˆå¹¶æ•°æ®
        all_data = business_data + formatted_open_source
        
        # æŒ‰é£æ ¼åˆ†ç»„
        grouped_data = defaultdict(list)
        for item in all_data:
            grouped_data[item['style']].append(item)
        
        # è®¡ç®—æ•°æ®åˆ†å¸ƒ
        total_count = len(all_data)
        business_count = len(business_data)
        opensource_count = len(formatted_open_source)
        
        print(f"âœ… æ•°æ®é›†æ•´åˆå®Œæˆ")
        print(f"   æ€»æ•°æ®é‡: {total_count} æ¡")
        print(f"   ä¸šåŠ¡æ•°æ®: {business_count} æ¡ ({business_count/total_count*100:.1f}%)")
        print(f"   å¼€æºæ•°æ®: {opensource_count} æ¡ ({opensource_count/total_count*100:.1f}%)")
        print("\n   å„é£æ ¼åˆ†å¸ƒ:")
        for style, items in grouped_data.items():
            print(f"     {style}: {len(items)} æ¡ ({len(items)/total_count*100:.1f}%)")
        
        # æ„é€ æœ€ç»ˆæ•°æ®é›†æ ¼å¼
        final_dataset = {
            'metadata': {
                'total_samples': total_count,
                'business_ratio': f"{business_count/total_count*100:.1f}%",
                'opensource_ratio': f"{opensource_count/total_count*100:.1f}%", 
                'creation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'style_distribution': {style: len(items) for style, items in grouped_data.items()},
                'processing_settings': self.processing_settings
            },
            'styles': {}
        }
        
        # æŒ‰é£æ ¼ç»„ç»‡æ•°æ®
        for style, items in grouped_data.items():
            style_english = self.style_to_english.get(style, style.lower())
            final_dataset['styles'][style] = {
                'style_name': style,
                'style_english': style_english,
                'sample_count': len(items),
                'samples': [
                    {
                        'user_query': item['user_query'],
                        'agent_reply': item['agent_reply'],
                        'source': item['source'],
                        'turn_id': item.get('turn_id', ''),
                        'scene': item.get('scene', '')
                    }
                    for item in items
                ]
            }
        
        return final_dataset
    
    def save_style_datasets(self, final_dataset: Dict):
        """
        ä¿å­˜å„é£æ ¼çš„ç‹¬ç«‹æ•°æ®æ–‡ä»¶
        """
        print("\nğŸ’¾ ä¿å­˜å„é£æ ¼ç‹¬ç«‹æ•°æ®æ–‡ä»¶...")
        
        for style, style_data in final_dataset['styles'].items():
            # ä¸ºæ¯ä¸ªé£æ ¼åˆ›å»ºç‹¬ç«‹çš„æ•°æ®æ–‡ä»¶
            style_filename = f"style_{style_data['style_english']}.json"
            style_filepath = os.path.join(self.output_data_dir, style_filename)
            
            style_dataset = {
                'metadata': {
                    'style_name': style,
                    'style_english': style_data['style_english'],
                    'sample_count': style_data['sample_count'],
                    'creation_time': final_dataset['metadata']['creation_time']
                },
                'samples': style_data['samples']
            }
            
            with open(style_filepath, 'w', encoding='utf-8') as f:
                json.dump(style_dataset, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… {style}({style_data['style_english']}): {style_filename}")
    
    def generate_training_format(self, dataset: Dict) -> List[Dict]:
        """
        ç”Ÿæˆæœ€ç»ˆçš„è®­ç»ƒæ ¼å¼æ•°æ®
        è½¬æ¢ä¸ºåˆ†ç±»æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ ¼å¼
        """
        print("\nğŸ¯ ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæ ¼å¼...")
        
        training_data = []
        
        for style, style_data in dataset['styles'].items():
            for sample in style_data['samples']:
                # ç»„åˆç”¨æˆ·é—®é¢˜å’Œå›ç­”ä½œä¸ºè¾“å…¥
                input_text = f"ç”¨æˆ·é—®é¢˜ï¼š{sample['user_query']}\nå›ç­”ï¼š{sample['agent_reply']}"
                
                # åˆ›å»ºè®­ç»ƒæ ·æœ¬
                training_sample = {
                    'input': input_text,
                    'label': style,
                    'label_english': style_data['style_english'],
                    'metadata': {
                        'source': sample['source'],
                        'scene': sample['scene'],
                        'turn_id': sample['turn_id']
                    }
                }
                training_data.append(training_sample)
        
        print(f"âœ… è®­ç»ƒæ ¼å¼ç”Ÿæˆå®Œæˆï¼Œå…± {len(training_data)} æ¡è®­ç»ƒæ ·æœ¬")
        
        # å±•ç¤ºæ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        label_counts = Counter([item['label'] for item in training_data])
        print("   æ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in label_counts.items():
            print(f"     {label}: {count} æ¡")
            
        return training_data
    
    def save_results(self, dataset: Dict, training_data: List[Dict]):
        """ä¿å­˜å¤„ç†ç»“æœåˆ°è¾“å‡ºç›®å½•"""
        print("\nğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ...")
        
        # ä¿å­˜å®Œæ•´æ•°æ®é›†
        final_dataset_path = os.path.join(self.output_data_dir, "final_dataset.json")
        with open(final_dataset_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒæ ¼å¼æ•°æ®
        training_data_path = os.path.join(self.output_data_dir, "training_data.json")
        with open(training_data_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å„é£æ ¼ç‹¬ç«‹æ•°æ®æ–‡ä»¶
        self.save_style_datasets(dataset)
        
        print("âœ… ç»“æœä¿å­˜å®Œæˆ")
        print(f"   ğŸ“„ final_dataset.json - å®Œæ•´æ•°æ®é›†")
        print(f"   ğŸ“„ training_data.json - è®­ç»ƒæ ¼å¼æ•°æ®") 
        print(f"   ğŸ“ style_*.json - å„é£æ ¼ç‹¬ç«‹æ•°æ®æ–‡ä»¶")
        print(f"   ğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {self.output_data_dir}")
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿"""
        print("ğŸ¬ å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿...")
        print("=" * 60)
        
        try:
            # æ­¥éª¤1: ä»æ–‡ä»¶åŠ è½½ä¸šåŠ¡æ•°æ®
            business_data = self.load_business_data()
            
            # æ­¥éª¤2: ä»æ–‡ä»¶åŠ è½½å¼€æºæ•°æ® 
            open_source_data = self.load_opensource_data()
            
            # æ­¥éª¤3: åœºæ™¯æ ‡ç­¾æ˜ å°„
            mapped_business_data = self.map_scenes_to_styles(business_data)
            
            # æ­¥éª¤4: è´¨é‡ç­›é€‰
            filtered_business_data = self.quality_filtering(mapped_business_data, "business")
            filtered_open_source_data = self.quality_filtering(open_source_data, "opensource")
            
            # æ­¥éª¤5: å¯¹è¯æ‹†åˆ†ï¼ˆä»…é’ˆå¯¹ä¸šåŠ¡æ•°æ®ï¼‰
            split_business_data = self.split_conversations(filtered_business_data)
            
            # æ­¥éª¤6: æ•°æ®æ¸…æ´—
            cleaned_business_data = self.clean_and_rewrite_data(split_business_data, "business")
            cleaned_open_source_data = self.clean_and_rewrite_data(filtered_open_source_data, "opensource")
            
            # æ­¥éª¤7: å»é‡å¤„ç†
            dedup_business_data = self.deduplicate_data(cleaned_business_data, "business")
            dedup_open_source_data = self.deduplicate_data(cleaned_open_source_data, "opensource")
            
            # æ­¥éª¤8: æ•´åˆæœ€ç»ˆæ•°æ®é›†
            final_dataset = self.integrate_final_dataset(dedup_business_data, dedup_open_source_data)
            
            # ç”Ÿæˆè®­ç»ƒæ ¼å¼
            training_data = self.generate_training_format(final_dataset)
            
            # ä¿å­˜ç»“æœ
            self.save_results(final_dataset, training_data)
            
            print("\n" + "=" * 60)
            print("ğŸ‰ æ•°æ®å¤„ç†æµæ°´çº¿å®Œæˆï¼")
            print("=" * 60)
            
            return final_dataset, training_data
            
        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            raise e


def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„åˆå§‹åŒ–å¤„ç†å™¨
    processor = RewardModelDataProcessor()
    final_dataset, training_data = processor.run_complete_pipeline()
    
    # å±•ç¤ºä¸€äº›æ ·æœ¬ç»“æœ
    print("\nğŸ“‹ è®­ç»ƒæ•°æ®æ ·æœ¬å±•ç¤º:")
    print("-" * 50)
    
    # å±•ç¤ºæ¯ç§é£æ ¼çš„ä¸€ä¸ªæ ·æœ¬
    shown_labels = set()
    for sample in training_data:
        label = sample['label']
        if label not in shown_labels:
            print(f"\nğŸ·ï¸  æ ‡ç­¾: {label} ({sample['label_english']})")
            print(f"ğŸ“ è¾“å…¥: {sample['input'][:100]}...")
            print(f"ğŸª æ¥æº: {sample['metadata']['source']}")
            shown_labels.add(label)
            
        if len(shown_labels) >= 5:  # å±•ç¤ºå‰5ç§ä¸åŒçš„æ ‡ç­¾
            break
    
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(training_data)}")
    print(f"   æ ‡ç­¾ç±»åˆ«æ•°: {len(set([item['label'] for item in training_data]))}")
    
    # å±•ç¤ºæ–‡ä»¶ç»“æ„
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„:")
    print(f"   {processor.output_data_dir}/")
    print(f"   â”œâ”€â”€ final_dataset.json")
    print(f"   â”œâ”€â”€ training_data.json")
    for style, style_data in final_dataset['styles'].items():
        style_english = style_data['style_english']
        print(f"   â””â”€â”€ style_{style_english}.json ({style})")


if __name__ == "__main__":
    main()
