#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥–åŠ±æ¨¡å‹æ¨ç†æ¼”ç¤ºè„šæœ¬
ç”¨äºæµ‹è¯•è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹çš„æ¨ç†æ•ˆæœ

ä½¿ç”¨æ–¹æ³•:
python inference_demo.py --model_path ./output/Qwen2.5-0.5B-Reward
"""

import torch
import json
import argparse
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification


class RewardModelInference:
    """å¥–åŠ±æ¨¡å‹æ¨ç†å™¨"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.load_model()
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        try:
            # åŠ è½½æ¨¡å‹å’Œtokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
            self.model.eval()
            
            # åŠ è½½æ ‡ç­¾æ˜ å°„
            label_mapping_path = Path(self.model_path) / "label_mapping.json"
            with open(label_mapping_path, 'r', encoding='utf-8') as f:
                label_mapping = json.load(f)
            
            self.label_to_id = label_mapping['label_to_id']
            self.id_to_label = {int(k): v for k, v in label_mapping['id_to_label'].items()}
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   æ”¯æŒçš„é£æ ¼ç±»åˆ«: {list(self.label_to_id.keys())}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def predict(self, text: str, max_length: int = 512) -> dict:
        """é¢„æµ‹å•æ¡æ–‡æœ¬çš„é£æ ¼"""
        # Tokenizeè¾“å…¥
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=max_length,
            truncation=True,
            padding="max_length"
        )
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1)
            predicted_class_id = torch.argmax(outputs.logits, dim=-1).item()
        
        # æ„å»ºç»“æœ
        predicted_label = self.id_to_label[predicted_class_id]
        confidence = probabilities[0][predicted_class_id].item()
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        all_probs = {}
        for class_id, prob in enumerate(probabilities[0]):
            class_name = self.id_to_label[class_id]
            all_probs[class_name] = prob.item()
        
        return {
            "predicted_label": predicted_label,
            "confidence": confidence,
            "probabilities": all_probs,
            "input_text": text[:100] + "..." if len(text) > 100 else text
        }
    
    def batch_predict(self, texts: list, max_length: int = 512) -> list:
        """æ‰¹é‡é¢„æµ‹å¤šæ¡æ–‡æœ¬"""
        results = []
        for text in texts:
            result = self.predict(text, max_length)
            results.append(result)
        return results
    
    def interactive_demo(self):
        """äº¤äº’å¼æ¼”ç¤º"""
        print("\nğŸ¯ äº¤äº’å¼æ¨ç†æ¼”ç¤º")
        print("=" * 60)
        print("è¾“å…¥æ ¼å¼: ç”¨æˆ·é—®é¢˜ï¼š[é—®é¢˜]\\nå›ç­”ï¼š[å›ç­”]")
        print("è¾“å…¥ 'quit' é€€å‡ºæ¼”ç¤º")
        print("")
        
        while True:
            try:
                text = input("è¯·è¾“å…¥æ–‡æœ¬: ")
                if text.lower() == 'quit':
                    break
                
                if not text.strip():
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
                    continue
                
                # é¢„æµ‹
                result = self.predict(text)
                
                # æ˜¾ç¤ºç»“æœ
                print("\nğŸ” é¢„æµ‹ç»“æœ:")
                print(f"   é¢„æµ‹é£æ ¼: {result['predicted_label']}")
                print(f"   ç½®ä¿¡åº¦: {result['confidence']:.3f}")
                print("   å„ç±»åˆ«æ¦‚ç‡:")
                
                # æŒ‰æ¦‚ç‡æ’åºæ˜¾ç¤º
                sorted_probs = sorted(result['probabilities'].items(), 
                                    key=lambda x: x[1], reverse=True)
                for style, prob in sorted_probs:
                    indicator = "ğŸ‘‘" if style == result['predicted_label'] else "  "
                    print(f"   {indicator} {style}: {prob:.3f}")
                print("")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ é€€å‡ºæ¼”ç¤º")
                break
            except Exception as e:
                print(f"âŒ é¢„æµ‹å‡ºé”™: {e}")
    
    def run_test_samples(self):
        """è¿è¡Œæµ‹è¯•æ ·ä¾‹"""
        print("\nğŸ§ª æµ‹è¯•æ ·ä¾‹æ¼”ç¤º")
        print("=" * 60)
        
        test_samples = [
            {
                "text": "ç”¨æˆ·é—®é¢˜ï¼šè¿™æ¬¾æ‰‹æœºæ€§èƒ½æ€ä¹ˆæ ·ï¼Ÿ\nå›ç­”ï¼šè¿™æ¬¾æ‰‹æœºæ‹¥æœ‰æœ€æ–°å¤„ç†å™¨ï¼Œæ€§èƒ½å¼ºåŠ²ï¼Œç°åœ¨è´­ä¹°è¿˜æœ‰é™æ—¶ä¼˜æƒ ï¼",
                "expected": "å”®å‰"
            },
            {
                "text": "ç”¨æˆ·é—®é¢˜ï¼šæˆ‘çš„è®¢å•æœ‰é—®é¢˜\nå›ç­”ï¼šéå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ï¼Œæˆ‘ç«‹å³ä¸ºæ‚¨æŸ¥è¯¢è®¢å•çŠ¶æ€å¹¶è§£å†³é—®é¢˜ã€‚",
                "expected": "å”®å"
            },
            {
                "text": "ç”¨æˆ·é—®é¢˜ï¼šè¿™ä¸ªäº§å“çœŸçš„æœ‰æ•ˆå—ï¼Ÿ\nå›ç­”ï¼šäº§å“æ•ˆæœå› äººè€Œå¼‚ï¼Œæˆ‘å»ºè®®æ‚¨æ ¹æ®è‡ªå·±çš„éœ€æ±‚åˆ¤æ–­ï¼Œæˆ‘ä»¬ä¼šå¦‚å®å‘ŠçŸ¥äº§å“çš„ä¼˜ç¼ºç‚¹ã€‚",
                "expected": "è¯šå®"
            },
            {
                "text": "ç”¨æˆ·é—®é¢˜ï¼šå¦‚ä½•è®¾ç½®å¯†ç ï¼Ÿ\nå›ç­”ï¼šæ‚¨å¯ä»¥è¿›å…¥è®¾ç½®-å®‰å…¨-å¯†ç ç®¡ç†ï¼ŒæŒ‰ç…§æ­¥éª¤è®¾ç½®æ–°å¯†ç ï¼Œå»ºè®®ä½¿ç”¨å¤æ‚å¯†ç ç¡®ä¿å®‰å…¨ã€‚",
                "expected": "æœ‰å¸®åŠ©"
            },
            {
                "text": "ç”¨æˆ·é—®é¢˜ï¼šæœ‰äººè¦æˆ‘çš„éªŒè¯ç \nå›ç­”ï¼šç»å¯¹ä¸è¦ç»™ä»»ä½•äººæ‚¨çš„éªŒè¯ç ï¼è¿™æ˜¯è¯ˆéª—è¡Œä¸ºï¼Œè¯·ç«‹å³æ‹’ç»å¹¶ä¿æŠ¤å¥½ä¸ªäººä¿¡æ¯ã€‚",
                "expected": "æ— ä¼¤å®³"
            }
        ]
        
        correct_predictions = 0
        
        for i, sample in enumerate(test_samples, 1):
            print(f"\nğŸ“ æµ‹è¯•æ ·ä¾‹ {i}:")
            print(f"æœŸæœ›é£æ ¼: {sample['expected']}")
            print(f"è¾“å…¥æ–‡æœ¬: {sample['text'][:100]}...")
            
            result = self.predict(sample['text'])
            is_correct = result['predicted_label'] == sample['expected']
            correct_predictions += is_correct
            
            status = "âœ…" if is_correct else "âŒ"
            print(f"é¢„æµ‹ç»“æœ: {result['predicted_label']} (ç½®ä¿¡åº¦: {result['confidence']:.3f}) {status}")
            
            if not is_correct:
                print("   æ‰€æœ‰æ¦‚ç‡:")
                sorted_probs = sorted(result['probabilities'].items(), 
                                    key=lambda x: x[1], reverse=True)
                for style, prob in sorted_probs[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    print(f"     {style}: {prob:.3f}")
        
        accuracy = correct_predictions / len(test_samples)
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        print(f"   æ­£ç¡®é¢„æµ‹: {correct_predictions}/{len(test_samples)}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.2%}")
        
        if accuracy >= 0.8:
            print("ğŸ‰ æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼")
        elif accuracy >= 0.6:
            print("ğŸ‘ æ¨¡å‹è¡¨ç°è‰¯å¥½")
        else:
            print("âš ï¸ æ¨¡å‹å¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")


def main():
    parser = argparse.ArgumentParser(description="å¥–åŠ±æ¨¡å‹æ¨ç†æ¼”ç¤º")
    parser.add_argument(
        "--model_path",
        type=str,
        default="./output/Qwen2.5-0.5B-Reward",
        help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["test", "interactive", "both"],
        default="both",
        help="è¿è¡Œæ¨¡å¼ï¼štest=æµ‹è¯•æ ·ä¾‹ï¼Œinteractive=äº¤äº’å¼ï¼Œboth=ä¸¤è€…éƒ½è¿è¡Œ"
    )
    parser.add_argument(
        "--max_length",
        type=int,
        default=512,
        help="æœ€å¤§åºåˆ—é•¿åº¦"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = Path(args.model_path)
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return
    
    print("ğŸ¯ å¥–åŠ±æ¨¡å‹æ¨ç†æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–æ¨ç†å™¨
        inference = RewardModelInference(args.model_path)
        
        # è¿è¡Œæµ‹è¯•
        if args.mode in ["test", "both"]:
            inference.run_test_samples()
        
        # è¿è¡Œäº¤äº’å¼æ¼”ç¤º
        if args.mode in ["interactive", "both"]:
            inference.interactive_demo()
        
    except Exception as e:
        print(f"âŒ æ¨ç†æ¼”ç¤ºå¤±è´¥: {e}")
        return
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()
