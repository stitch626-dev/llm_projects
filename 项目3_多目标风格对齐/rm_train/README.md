# å¥–åŠ±æ¨¡å‹è®­ç»ƒç³»ç»Ÿ

åŸºäºTRLåº“å®ç°çš„5ç±»é£æ ¼å¥–åŠ±æ¨¡å‹è®­ç»ƒç³»ç»Ÿï¼Œç”¨äºè®­ç»ƒèƒ½å¤Ÿè¯†åˆ«**å”®å‰**ã€**å”®å**ã€**è¯šå®**ã€**æœ‰å¸®åŠ©**ã€**æ— ä¼¤å®³**äº”ç§å¯¹è¯é£æ ¼çš„åˆ†ç±»æ¨¡å‹ã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

è¿™ä¸ªè®­ç»ƒç³»ç»Ÿå°†æˆ‘ä»¬æ„é€ çš„å¥–åŠ±æ¨¡å‹æ•°æ®é›†è½¬æ¢ä¸ºä¸€ä¸ªå®ç”¨çš„åˆ†ç±»æ¨¡å‹ï¼Œå¯ä»¥ï¼š

- **è¾“å…¥**: ç”¨æˆ·é—®é¢˜ + AIå›ç­”çš„ç»„åˆæ–‡æœ¬
- **è¾“å‡º**: 5ç§é£æ ¼çš„åˆ†ç±»æ¦‚ç‡å’Œç½®ä¿¡åº¦
- **åº”ç”¨**: å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ä¿¡å·ï¼Œå†…å®¹é£æ ¼æ§åˆ¶ï¼Œè´¨é‡è¯„ä¼°ç­‰

### æ”¯æŒçš„é£æ ¼ç±»åˆ«

| ä¸­æ–‡æ ‡ç­¾ | è‹±æ–‡æ ‡è¯† | ç±»åˆ«ID | æè¿° |
|---------|---------|--------|------|
| å”®å‰ | sales_oriented | 0 | é”€å”®å¯¼å‘ï¼Œæ¨èäº§å“ï¼Œä¿ƒè¿›è´­ä¹° |
| å”®å | after_sales | 1 | å”®åæœåŠ¡ï¼Œè§£å†³é—®é¢˜ï¼Œå¤„ç†æŠ•è¯‰ |
| è¯šå® | honesty | 2 | è¯šå®å®¢è§‚ï¼Œå®äº‹æ±‚æ˜¯ï¼Œæ‰¿è®¤é™åˆ¶ |
| æœ‰å¸®åŠ© | helpful | 3 | æä¾›å¸®åŠ©ï¼Œè§£ç­”ç–‘é—®ï¼Œç»™å‡ºå»ºè®® |
| æ— ä¼¤å®³ | harmless | 4 | å®‰å…¨æ— å®³ï¼Œæ‹’ç»å±é™©ï¼Œä¿æŠ¤ç”¨æˆ· |

## ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

### ä¾èµ–åº“
```bash
pip install torch transformers trl datasets scikit-learn numpy
```

### æµ‹è¯•ç¯å¢ƒ
- Python 3.8+
- PyTorch 2.0+
- Transformers 4.30+
- TRL 0.7+

## ğŸ“ æ–‡ä»¶ç»“æ„

```
rm_train/
â”œâ”€â”€ rm_train.py          # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ README.md            # æœ¬æ–‡æ¡£
â”œâ”€â”€ requirements.txt     # ä¾èµ–åˆ—è¡¨
â””â”€â”€ output/             # è®­ç»ƒè¾“å‡ºç›®å½•
    â””â”€â”€ Qwen2.5-0.5B-Reward/  # è®­ç»ƒå¥½çš„æ¨¡å‹
        â”œâ”€â”€ config.json
        â”œâ”€â”€ pytorch_model.bin
        â”œâ”€â”€ tokenizer.json
        â””â”€â”€ label_mapping.json
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬è®­ç»ƒ
```bash
cd rm_train
python rm_train.py
```

### 2. è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
```bash
python rm_train.py \
    --model_name_or_path "Qwen/Qwen2.5-0.5B-Instruct" \
    --train_file "../rm_data/output_data/training_data.json" \
    --output_dir "./output/my_reward_model" \
    --num_train_epochs 5 \
    --per_device_train_batch_size 8 \
    --learning_rate 2e-5 \
    --demo_inference
```

## âš™ï¸ è®­ç»ƒå‚æ•°è¯´æ˜

### æ ¸å¿ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--model_name_or_path` | Qwen/Qwen2.5-0.5B-Instruct | åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹ |
| `--train_file` | rm_data/output_data/training_data.json | è®­ç»ƒæ•°æ®æ–‡ä»¶ |
| `--output_dir` | rm_train/output/Qwen2.5-0.5B-Reward | æ¨¡å‹è¾“å‡ºç›®å½• |
| `--max_length` | 512 | æœ€å¤§è¾“å…¥åºåˆ—é•¿åº¦ |
| `--num_train_epochs` | 3 | è®­ç»ƒè½®æ•° |
| `--learning_rate` | 2e-5 | å­¦ä¹ ç‡ |

### æ€§èƒ½å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--per_device_train_batch_size` | 4 | æ¯è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| `--per_device_eval_batch_size` | 8 | æ¯è®¾å¤‡è¯„ä¼°æ‰¹æ¬¡å¤§å° |
| `--weight_decay` | 0.01 | æƒé‡è¡°å‡ |
| `--seed` | 42 | éšæœºç§å­ |

### åŠŸèƒ½å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--demo_inference` | False | è®­ç»ƒåæ‰§è¡Œæ¨ç†æ¼”ç¤º |

## ğŸ“Š è®­ç»ƒè¿‡ç¨‹

### 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
```
ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: rm_data/output_data/training_data.json
âœ… æˆåŠŸåŠ è½½ 26 æ¡è®­ç»ƒæ ·æœ¬
ğŸ“Š è®­ç»ƒæ•°æ®æ ‡ç­¾åˆ†å¸ƒ:
   å”®å‰: 7 æ¡
   å”®å: 4 æ¡
   æœ‰å¸®åŠ©: 6 æ¡
   æ— ä¼¤å®³: 4 æ¡
   è¯šå®: 5 æ¡
ğŸ“Š æ•°æ®åˆ’åˆ†:
   è®­ç»ƒé›†: 20 æ¡
   éªŒè¯é›†: 6 æ¡
```

### 2. æ¨¡å‹åˆå§‹åŒ–
```
ğŸ¤– åˆå§‹åŒ–æ¨¡å‹: Qwen/Qwen2.5-0.5B-Instruct
âœ… æ¨¡å‹åŠ è½½å®Œæˆ
   æ¨¡å‹ç±»åˆ«æ•°: 5
   è¯æ±‡è¡¨å¤§å°: 151936
```

### 3. è®­ç»ƒæŒ‡æ ‡ç›‘æ§
- **Accuracy**: åˆ†ç±»å‡†ç¡®ç‡
- **F1 Macro**: å®å¹³å‡F1åˆ†æ•°
- **F1 Weighted**: åŠ æƒF1åˆ†æ•°
- **Per Class F1**: æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°

## ğŸ“ˆ æ¨¡å‹è¾“å‡º

### 1. è®­ç»ƒç»“æœç¤ºä¾‹
```
ğŸ“Š è®­ç»ƒåæ¨¡å‹è¯„ä¼°:
   eval_accuracy: 0.8333
   eval_f1_macro: 0.8200
   eval_f1_weighted: 0.8300
   eval_per_class_f1: {
     'å”®å‰': 0.8571,
     'å”®å': 0.7500,
     'è¯šå®': 0.8000,
     'æœ‰å¸®åŠ©': 0.8889,
     'æ— ä¼¤å®³': 0.8000
   }
```

### 2. æ¨ç†è¾“å‡ºæ ¼å¼
```python
# å•æ¡æ¨ç†ç»“æœ
{
    "predicted_label": "è¯šå®",
    "confidence": 0.856,
    "probabilities": {
        "å”®å‰": 0.045,
        "å”®å": 0.032,
        "è¯šå®": 0.856,
        "æœ‰å¸®åŠ©": 0.054,
        "æ— ä¼¤å®³": 0.013
    }
}
```

### 3. æ–‡ä»¶è¾“å‡º
- `config.json`: æ¨¡å‹é…ç½®æ–‡ä»¶
- `pytorch_model.bin`: è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
- `tokenizer.json`: åˆ†è¯å™¨é…ç½®
- `label_mapping.json`: æ ‡ç­¾æ˜ å°„å…³ç³»

## ğŸ¯ æ¨¡å‹æ¨ç†

### 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import json

# åŠ è½½æ¨¡å‹å’Œtokenizer
model_path = "rm_train/output/Qwen2.5-0.5B-Reward"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# åŠ è½½æ ‡ç­¾æ˜ å°„
with open(f"{model_path}/label_mapping.json", 'r') as f:
    label_mapping = json.load(f)
id_to_label = label_mapping['id_to_label']
```

### 2. æ¨ç†å‡½æ•°
```python
def predict_style(text, model, tokenizer, id_to_label, max_length=512):
    """é¢„æµ‹æ–‡æœ¬çš„é£æ ¼ç±»åˆ«"""
    # Tokenizeè¾“å…¥
    inputs = tokenizer(
        text, 
        return_tensors="pt", 
        max_length=max_length,
        truncation=True, 
        padding="max_length"
    )
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        probabilities = torch.softmax(outputs.logits, dim=-1)
        predicted_class_id = torch.argmax(outputs.logits, dim=-1).item()
    
    # æ„å»ºç»“æœ
    predicted_label = id_to_label[str(predicted_class_id)]
    confidence = probabilities[0][predicted_class_id].item()
    
    all_probs = {}
    for i, prob in enumerate(probabilities[0]):
        all_probs[id_to_label[str(i)]] = prob.item()
    
    return {
        "predicted_label": predicted_label,
        "confidence": confidence,
        "probabilities": all_probs
    }
```

### 3. ä½¿ç”¨ç¤ºä¾‹
```python
# æµ‹è¯•æ ·ä¾‹
test_text = "ç”¨æˆ·é—®é¢˜ï¼šè¿™ä¸ªäº§å“æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ\nå›ç­”ï¼šè¿™ä¸ªäº§å“ç¡®å®æœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œå»ºè®®æ‚¨æ ¹æ®è‡ªå·±çš„éœ€æ±‚æ¥åˆ¤æ–­æ˜¯å¦åˆé€‚ã€‚"

result = predict_style(test_text, model, tokenizer, id_to_label)
print(f"é¢„æµ‹é£æ ¼: {result['predicted_label']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
print("å„ç±»åˆ«æ¦‚ç‡:")
for style, prob in result['probabilities'].items():
    print(f"  {style}: {prob:.3f}")
```

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### 1. ä¿®æ”¹è®­ç»ƒæ•°æ®
å¦‚æœè¦ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Œç¡®ä¿JSONæ ¼å¼å¦‚ä¸‹ï¼š
```json
[
  {
    "input": "ç”¨æˆ·é—®é¢˜ï¼š[é—®é¢˜]\nå›ç­”ï¼š[å›ç­”]",
    "label": "å”®å‰",
    "label_english": "sales_oriented",
    "metadata": {
      "source": "business",
      "scene": "å•†å“å’¨è¯¢"
    }
  }
]
```

### 2. æ·»åŠ æ–°çš„é£æ ¼ç±»åˆ«
1. ä¿®æ”¹ `rm_train.py` ä¸­çš„ `label_to_id` æ˜ å°„
2. æ›´æ–°è®­ç»ƒæ•°æ®ä¸­çš„æ ‡ç­¾
3. é‡æ–°è®­ç»ƒæ¨¡å‹

### 3. è°ƒæ•´æ¨¡å‹æ¶æ„
- æ›´æ¢åŸºç¡€æ¨¡å‹ï¼šä¿®æ”¹ `--model_name_or_path` å‚æ•°
- è°ƒæ•´åºåˆ—é•¿åº¦ï¼šä¿®æ”¹ `--max_length` å‚æ•°
- ä¼˜åŒ–è¶…å‚æ•°ï¼šè°ƒæ•´å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰


## â“ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A**: å‡å°æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦
```bash
--per_device_train_batch_size 2 \
--max_length 256
```

### Q2: å¦‚ä½•æé«˜æ¨¡å‹æ€§èƒ½ï¼Ÿ
**A**: 
- å¢åŠ è®­ç»ƒè½®æ•°ï¼š`--num_train_epochs 5`
- è°ƒæ•´å­¦ä¹ ç‡ï¼š`--learning_rate 1e-5`
- ä½¿ç”¨æ›´å¤§çš„åŸºç¡€æ¨¡å‹
- å¢åŠ è®­ç»ƒæ•°æ®é‡

### Q3: å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ
**A**: 
- åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µè¿›è¡Œæ•°æ®å¢å¼º
- ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡æŸå¤±å‡½æ•°
- è°ƒæ•´è®­ç»ƒæ ·æœ¬çš„é‡‡æ ·ç­–ç•¥

### Q4: è®­ç»ƒæ—¶é—´å¤ªé•¿æ€ä¹ˆåŠï¼Ÿ
**A**:
- ä½¿ç”¨GPUåŠ é€Ÿ
- å‡å°‘è®­ç»ƒè½®æ•°å’Œåºåˆ—é•¿åº¦
- ä½¿ç”¨æ›´å°çš„åŸºç¡€æ¨¡å‹

### Q5: å¦‚ä½•è¯„ä¼°æ¨¡å‹è´¨é‡ï¼Ÿ
**A**:
- å…³æ³¨F1åˆ†æ•°è€Œä¸ä»…ä»…æ˜¯å‡†ç¡®ç‡
- æŸ¥çœ‹æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
- ä½¿ç”¨æ··æ·†çŸ©é˜µåˆ†æé”™è¯¯ç±»å‹
- è¿›è¡Œäººå·¥è¯„ä¼°éªŒè¯

## ğŸš€ è¿›é˜¶åŠŸèƒ½

### 1. é›†æˆåˆ°æ¨ç†æœåŠ¡
```python
from fastapi import FastAPI
import torch

app = FastAPI()
model = None
tokenizer = None

@app.on_event("startup")
async def load_model():
    global model, tokenizer
    model = AutoModelForSequenceClassification.from_pretrained("./output/Qwen2.5-0.5B-Reward")
    tokenizer = AutoTokenizer.from_pretrained("./output/Qwen2.5-0.5B-Reward")

@app.post("/predict")
async def predict(text: str):
    result = predict_style(text, model, tokenizer, id_to_label)
    return result
```

### 2. æ‰¹é‡æ¨ç†
```python
def batch_predict(texts, model, tokenizer, id_to_label, batch_size=32):
    """æ‰¹é‡é¢„æµ‹å¤šä¸ªæ–‡æœ¬çš„é£æ ¼"""
    results = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        # æ‰¹é‡å¤„ç†é€»è¾‘...
        # è¿”å›æ‰¹é‡ç»“æœ
    return results
```

### 3. æ¨¡å‹è’¸é¦
ä½¿ç”¨è®­ç»ƒå¥½çš„å¤§æ¨¡å‹æ¥æŒ‡å¯¼æ›´å°æ¨¡å‹çš„è®­ç»ƒï¼Œæé«˜æ¨ç†é€Ÿåº¦ã€‚

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. **ç¯å¢ƒä¾èµ–**: ç¡®ä¿æ‰€æœ‰ä¾èµ–åº“ç‰ˆæœ¬æ­£ç¡®
2. **æ•°æ®æ ¼å¼**: éªŒè¯è®­ç»ƒæ•°æ®çš„JSONæ ¼å¼
3. **ç¡¬ä»¶èµ„æº**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUæ˜¾å­˜æˆ–CPUå†…å­˜
4. **æ¨¡å‹æƒé™**: ç¡®ä¿èƒ½å¤Ÿè®¿é—®Hugging Faceæ¨¡å‹

æ›´å¤šæŠ€æœ¯ç»†èŠ‚å’Œæœ€ä½³å®è·µï¼Œè¯·å‚è€ƒé¡¹ç›®ä»£ç å’Œæ³¨é‡Šã€‚

---

ğŸ‰ **ç¥æ‚¨è®­ç»ƒé¡ºåˆ©ï¼** å¦‚æœè¿™ä¸ªå¥–åŠ±æ¨¡å‹åœ¨æ‚¨çš„é¡¹ç›®ä¸­å‘æŒ¥äº†ä½œç”¨ï¼Œæ¬¢è¿åˆ†äº«ä½¿ç”¨ä½“éªŒå’Œæ”¹è¿›å»ºè®®ã€‚
