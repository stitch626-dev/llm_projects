#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥–åŠ±æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆTRLç‰ˆæœ¬ï¼‰
ä½¿ç”¨TRLåº“è®­ç»ƒ5ç±»é£æ ¼åˆ†ç±»çš„å¥–åŠ±æ¨¡å‹ï¼šå”®å‰ã€å”®åã€è¯šå®ã€æœ‰å¸®åŠ©ã€æ— ä¼¤å®³

ç‰¹ç‚¹ï¼š
- ä½¿ç”¨TRLåº“çš„RewardConfigå’ŒRewardTrainer
- åŸºäºQwen2.5-0.5B-Instructæ¨¡å‹
- æ”¯æŒå¤šåˆ†ç±»ä»»åŠ¡ï¼ˆ5ä¸ªé£æ ¼ç±»åˆ«ï¼‰
- é›†æˆæˆ‘ä»¬æ„é€ çš„æ•°æ®é›†
"""

import torch
import json
import os
from typing import Dict, List, Any
import argparse
from pathlib import Path

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
)
from trl import RewardConfig, RewardTrainer
from datasets import Dataset, DatasetDict
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, classification_report


class RewardModelTrainer:
    """å¥–åŠ±æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        
        # é£æ ¼æ ‡ç­¾æ˜ å°„
        self.label_to_id = {
            "å”®å‰": 0,
            "å”®å": 1, 
            "è¯šå®": 2,
            "æœ‰å¸®åŠ©": 3,
            "æ— ä¼¤å®³": 4
        }
        self.id_to_label = {v: k for k, v in self.label_to_id.items()}
        
        print("ğŸš€ å¥–åŠ±æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š æ”¯æŒçš„é£æ ¼ç±»åˆ«: {list(self.label_to_id.keys())}")
        print(f"ğŸ”¢ ç±»åˆ«æ•°é‡: {len(self.label_to_id)}")
    
    def load_dataset(self) -> DatasetDict:
        """åŠ è½½æˆ‘ä»¬æ„é€ çš„è®­ç»ƒæ•°æ®"""
        print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {self.args.train_file}")
        
        try:
            with open(self.args.train_file, 'r', encoding='utf-8') as f:
                train_data = json.load(f)
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(train_data)} æ¡è®­ç»ƒæ ·æœ¬")
            
            # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
            label_counts = {}
            for item in train_data:
                label = item['label']
                label_counts[label] = label_counts.get(label, 0) + 1
            
            print("ğŸ“Š è®­ç»ƒæ•°æ®æ ‡ç­¾åˆ†å¸ƒ:")
            for label, count in label_counts.items():
                print(f"   {label}: {count} æ¡")
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            processed_data = []
            for item in train_data:
                processed_item = {
                    'text': item['input'],  # TRLæœŸæœ›çš„è¾“å…¥å­—æ®µå
                    'label': self.label_to_id[item['label']],  # è½¬æ¢ä¸ºæ•°å€¼æ ‡ç­¾
                    'label_text': item['label'],  # ä¿ç•™åŸå§‹æ ‡ç­¾ç”¨äºè°ƒè¯•
                    'source': item['metadata']['source']
                }
                processed_data.append(processed_item)
            
            # åˆ›å»ºè®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ï¼ˆ80/20ï¼‰
            split_idx = int(len(processed_data) * 0.8)
            train_split = processed_data[:split_idx]
            val_split = processed_data[split_idx:]
            
            dataset = DatasetDict({
                'train': Dataset.from_list(train_split),
                'validation': Dataset.from_list(val_split)
            })
            
            print(f"ğŸ“Š æ•°æ®åˆ’åˆ†:")
            print(f"   è®­ç»ƒé›†: {len(dataset['train'])} æ¡")
            print(f"   éªŒè¯é›†: {len(dataset['validation'])} æ¡")
            
            return dataset
            
        except FileNotFoundError:
            print(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {self.args.train_file}")
            raise
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
            raise
    
    def preprocess_function(self, examples):
        """æ•°æ®é¢„å¤„ç†å‡½æ•°"""
        # å¯¹æ–‡æœ¬è¿›è¡Œtokenize
        model_inputs = self.tokenizer(
            examples["text"],
            max_length=self.args.max_length,
            truncation=True,
            padding="max_length",
            return_tensors=None
        )
        
        # æ·»åŠ æ ‡ç­¾
        model_inputs["labels"] = examples["label"]
        return model_inputs
    
    def compute_metrics(self, eval_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        accuracy = accuracy_score(labels, predictions)
        f1_macro = f1_score(labels, predictions, average='macro')
        f1_weighted = f1_score(labels, predictions, average='weighted')
        
        # è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
        target_names = [self.id_to_label[i] for i in range(len(self.id_to_label))]
        class_report = classification_report(
            labels, predictions, 
            target_names=target_names, 
            output_dict=True, 
            zero_division=0
        )
        
        return {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'per_class_f1': {name: class_report[name]['f1-score'] 
                            for name in target_names if name in class_report}
        }
    
    def setup_model_and_tokenizer(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer"""
        print(f"\nğŸ¤– åˆå§‹åŒ–æ¨¡å‹: {self.args.model_name_or_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.args.model_name_or_path,
            trust_remote_code=True
        )
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹ï¼ˆç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.args.model_name_or_path,
            num_labels=len(self.label_to_id),  # 5ä¸ªç±»åˆ«
            trust_remote_code=True
        )
        
        # è®¾ç½®æ¨¡å‹çš„pad_token_id
        self.model.config.pad_token_id = self.tokenizer.pad_token_id
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   æ¨¡å‹ç±»åˆ«æ•°: {self.model.config.num_labels}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹...")
        
        # 1. åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
        self.setup_model_and_tokenizer()
        
        # 2. åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset()
        
        # 3. æ•°æ®é¢„å¤„ç†
        print("\nğŸ”§ é¢„å¤„ç†æ•°æ®...")
        tokenized_dataset = dataset.map(
            self.preprocess_function,
            batched=True,
            remove_columns=dataset["train"].column_names
        )
        
        # 4. è®¾ç½®è®­ç»ƒå‚æ•°
        training_args = RewardConfig(
            output_dir=self.args.output_dir,
            per_device_train_batch_size=self.args.per_device_train_batch_size,
            per_device_eval_batch_size=self.args.per_device_eval_batch_size,
            eval_strategy="epoch",
            save_strategy="epoch",
            learning_rate=self.args.learning_rate,
            num_train_epochs=self.args.num_train_epochs,
            weight_decay=self.args.weight_decay,
            logging_dir=os.path.join(self.args.output_dir, "logs"),
            logging_steps=10,
            fp16=True if torch.cuda.is_available() else False,
            save_safetensors=True,
            load_best_model_at_end=True,
            metric_for_best_model="eval_f1_macro",
            greater_is_better=True,
            report_to="none",
            remove_unused_columns=False,
        )
        
        # 5. åˆ›å»ºè®­ç»ƒå™¨
        trainer = RewardTrainer(
            args=training_args,
            model=self.model,
            processing_class=self.tokenizer,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset["validation"],
            compute_metrics=self.compute_metrics,
        )
        
        # 6. è®­ç»ƒå‰è¯„ä¼°
        print("\nğŸ“Š è®­ç»ƒå‰æ¨¡å‹è¯„ä¼°:")
        initial_metrics = trainer.evaluate()
        for key, value in initial_metrics.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.4f}")
            else:
                print(f"   {key}: {value}")
        
        # 7. å¼€å§‹è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # 8. è®­ç»ƒåè¯„ä¼°
        print("\nğŸ“Š è®­ç»ƒåæ¨¡å‹è¯„ä¼°:")
        final_metrics = trainer.evaluate()
        for key, value in final_metrics.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.4f}")
            else:
                print(f"   {key}: {value}")
        
        # 9. ä¿å­˜æ¨¡å‹
        print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {self.args.output_dir}")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.args.output_dir)
        
        # 10. ä¿å­˜æ ‡ç­¾æ˜ å°„
        label_mapping_path = os.path.join(self.args.output_dir, "label_mapping.json")
        with open(label_mapping_path, 'w', encoding='utf-8') as f:
            json.dump({
                'label_to_id': self.label_to_id,
                'id_to_label': self.id_to_label
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ ‡ç­¾æ˜ å°„å·²ä¿å­˜åˆ°: {label_mapping_path}")
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        return final_metrics
    
    def demo_inference(self):
        """æ¼”ç¤ºæ¨ç†è¿‡ç¨‹"""
        print("\nğŸ¯ æ¼”ç¤ºæ¨¡å‹æ¨ç†...")
        
        # æµ‹è¯•æ ·ä¾‹
        test_samples = [
            "ç”¨æˆ·é—®é¢˜ï¼šè¿™æ¬¾æ‰‹æœºæ€§èƒ½æ€ä¹ˆæ ·ï¼Ÿ\nå›ç­”ï¼šè¿™æ¬¾æ‰‹æœºæ‹¥æœ‰æœ€æ–°å¤„ç†å™¨ï¼Œæ€§èƒ½å¼ºåŠ²ï¼Œç°åœ¨è´­ä¹°è¿˜æœ‰é™æ—¶ä¼˜æƒ ï¼",
            "ç”¨æˆ·é—®é¢˜ï¼šæˆ‘çš„è®¢å•æœ‰é—®é¢˜\nå›ç­”ï¼šéå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ï¼Œæˆ‘ç«‹å³ä¸ºæ‚¨æŸ¥è¯¢è®¢å•çŠ¶æ€å¹¶è§£å†³é—®é¢˜ã€‚",
            "ç”¨æˆ·é—®é¢˜ï¼šè¿™ä¸ªäº§å“çœŸçš„æœ‰æ•ˆå—ï¼Ÿ\nå›ç­”ï¼šäº§å“æ•ˆæœå› äººè€Œå¼‚ï¼Œæˆ‘å»ºè®®æ‚¨æ ¹æ®è‡ªå·±çš„éœ€æ±‚åˆ¤æ–­ï¼Œæˆ‘ä»¬ä¼šå¦‚å®å‘ŠçŸ¥äº§å“çš„ä¼˜ç¼ºç‚¹ã€‚",
            "ç”¨æˆ·é—®é¢˜ï¼šå¦‚ä½•è®¾ç½®å¯†ç ï¼Ÿ\nå›ç­”ï¼šæ‚¨å¯ä»¥è¿›å…¥è®¾ç½®-å®‰å…¨-å¯†ç ç®¡ç†ï¼ŒæŒ‰ç…§æ­¥éª¤è®¾ç½®æ–°å¯†ç ï¼Œå»ºè®®ä½¿ç”¨å¤æ‚å¯†ç ç¡®ä¿å®‰å…¨ã€‚",
            "ç”¨æˆ·é—®é¢˜ï¼šæœ‰äººè¦æˆ‘çš„éªŒè¯ç \nå›ç­”ï¼šç»å¯¹ä¸è¦ç»™ä»»ä½•äººæ‚¨çš„éªŒè¯ç ï¼è¿™æ˜¯è¯ˆéª—è¡Œä¸ºï¼Œè¯·ç«‹å³æ‹’ç»å¹¶ä¿æŠ¤å¥½ä¸ªäººä¿¡æ¯ã€‚"
        ]
        
        expected_labels = ["å”®å‰", "å”®å", "è¯šå®", "æœ‰å¸®åŠ©", "æ— ä¼¤å®³"]
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†
        try:
            model = AutoModelForSequenceClassification.from_pretrained(self.args.output_dir)
            tokenizer = AutoTokenizer.from_pretrained(self.args.output_dir)
            model.eval()
            
            print("ğŸ” æ¨ç†ç»“æœ:")
            print("-" * 80)
            
            for i, (sample, expected) in enumerate(zip(test_samples, expected_labels)):
                # Tokenizeè¾“å…¥
                inputs = tokenizer(
                    sample, 
                    return_tensors="pt", 
                    max_length=self.args.max_length,
                    truncation=True, 
                    padding="max_length"
                )
                
                # é¢„æµ‹
                with torch.no_grad():
                    outputs = model(**inputs)
                    probabilities = torch.softmax(outputs.logits, dim=-1)
                    predicted_class_id = torch.argmax(outputs.logits, dim=-1).item()
                
                predicted_label = self.id_to_label[predicted_class_id]
                confidence = probabilities[0][predicted_class_id].item()
                
                # æ˜¾ç¤ºç»“æœ
                print(f"\næ ·æœ¬ {i+1}: {expected} (æœŸæœ›)")
                print(f"è¾“å…¥: {sample[:100]}...")
                print(f"é¢„æµ‹: {predicted_label} (ç½®ä¿¡åº¦: {confidence:.3f})")
                print(f"æ­£ç¡®: {'âœ…' if predicted_label == expected else 'âŒ'}")
                
                # æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
                print("å„ç±»åˆ«æ¦‚ç‡:")
                for class_id, prob in enumerate(probabilities[0]):
                    class_name = self.id_to_label[class_id]
                    print(f"  {class_name}: {prob:.3f}")
        
        except Exception as e:
            print(f"âš ï¸ æ¨ç†æ¼”ç¤ºå¤±è´¥: {e}")
            print("è¯·ç¡®ä¿æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶æ­£ç¡®ä¿å­˜")


def main():
    parser = argparse.ArgumentParser(description="å¥–åŠ±æ¨¡å‹è®­ç»ƒè„šæœ¬")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model_name_or_path", 
        type=str, 
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„"
    )
    
    # æ•°æ®å‚æ•°
    parser.add_argument(
        "--train_file", 
        type=str, 
        default="rm_data/output_data/training_data.json",
        help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„"
    )
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--output_dir", type=str, default="rm_train/output/Qwen2.5-0.5B-Reward", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--per_device_train_batch_size", type=int, default=4, help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--per_device_eval_batch_size", type=int, default=8, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--num_train_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--demo_inference", action="store_true", help="è®­ç»ƒåè¿›è¡Œæ¨ç†æ¼”ç¤º")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("ğŸ† å¥–åŠ±æ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    print("=" * 80)
    print(f"ğŸ¤– åŸºç¡€æ¨¡å‹: {args.model_name_or_path}")
    print(f"ğŸ“‚ è®­ç»ƒæ•°æ®: {args.train_file}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"âš™ï¸  è®­ç»ƒè½®æ•°: {args.num_train_epochs}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {args.per_device_train_batch_size}")
    print(f"ğŸ¯ æœ€å¤§é•¿åº¦: {args.max_length}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = RewardModelTrainer(args)
    metrics = trainer.train()
    
    # å¯é€‰çš„æ¨ç†æ¼”ç¤º
    if args.demo_inference:
        trainer.demo_inference()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()